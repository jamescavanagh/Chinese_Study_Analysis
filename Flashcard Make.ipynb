{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import pinyin\n",
    "import translators\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PathForNotes = Path('/home/jentlejames/Downloads') \n",
    "\n",
    "\n",
    "# Target for automation\n",
    "fileName = 'WF - Notes - 220722-110837.opml'\n",
    "\n",
    "\n",
    "main_vocab_df = pd.read_csv('./vocab_master.csv',index_col=0)\n",
    "\n",
    "# Chinese Japanese Character Ranges\n",
    "cjk_ranges = [\n",
    "    ( 0x4E00,  0x62FF),\n",
    "    ( 0x6300,  0x77FF),\n",
    "    ( 0x7800,  0x8CFF),\n",
    "    ( 0x8D00,  0x9FCC),\n",
    "    ( 0x3400,  0x4DB5),\n",
    "    (0x20000, 0x215FF),\n",
    "    (0x21600, 0x230FF),\n",
    "    (0x23100, 0x245FF),\n",
    "    (0x24600, 0x260FF),\n",
    "    (0x26100, 0x275FF),\n",
    "    (0x27600, 0x290FF),\n",
    "    (0x29100, 0x2A6DF),\n",
    "    (0x2A700, 0x2B734),\n",
    "    (0x2B740, 0x2B81D),\n",
    "    (0x2B820, 0x2CEAF),\n",
    "    (0x2CEB0, 0x2EBEF),\n",
    "    (0x2F800, 0x2FA1F),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def is_cjk(char):\n",
    "    char = ord(char)\n",
    "    for bottom, top in cjk_ranges:\n",
    "        if char >= bottom and char <= top:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "\n",
    "file_opml = PathForNotes/fileName         #sys.argv[1]\n",
    "\n",
    "\n",
    "with open(file_opml, 'rt') as f:\n",
    "    tree = ElementTree.parse(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drills down the the relevant element on the tree for Chinese notes\n",
    "# In future can change to find automatically \n",
    "\n",
    "notesRoot = tree.getroot()\n",
    "notesBody = notesRoot.find('body')\n",
    "outlineRoot = notesBody.find('outline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Looks for element text which matches internal \n",
    "# workflowy pattern in opml \n",
    "timePattern = re.compile('time startYear')\n",
    "\n",
    "# Creating list of elements\n",
    "# Which contain lesson notes \n",
    "elementsByDate = []\n",
    "elementsDateAdded = []\n",
    "\n",
    "\n",
    "# Checks for elements which contain dates\n",
    "# Which will contain sub elements with vocab\n",
    "# Appends them to a list, along with the dates\n",
    "# From before\n",
    "for outline in outlineRoot.iter():\n",
    "\n",
    "    text = outline.attrib['text']\n",
    "    \n",
    "    #Extracts the Date \n",
    "    # finds new nodes\n",
    "    \n",
    "    hasTime = re.search(timePattern,text)\n",
    "    \n",
    "    if hasTime:\n",
    "        #Extracting datetime info\n",
    "        time = text.split()\n",
    "        year = time[1].split('\\\"')[1]\n",
    "        month = time[2].split('\\\"')[1]\n",
    "        day = time[3].split('\\\"')[1]\n",
    "        dateAdded = year+' '+month +' '+day\n",
    "        #print(dateAdded)\n",
    "        \n",
    "        elementsByDate.append(outline)\n",
    "        elementsDateAdded.append(dateAdded)\n",
    "    else:\n",
    "        continue \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(elementsByDate)\n",
    "\n",
    "# Creates a dataframe of all the vocabulary accummulated\n",
    "# In lessons\n",
    "\n",
    "vocab_df = pd.DataFrame({},columns=['Characters','Date'])\n",
    "\n",
    "for i, lessonNotes in enumerate(elementsByDate):\n",
    "    \n",
    "    vocabFromLesson = []\n",
    "    \n",
    "    #print(elementsDateAdded[i])\n",
    "    for note in lessonNotes:\n",
    "        noteText = note.attrib['text']\n",
    "        \n",
    "        chineseCharacters = ''\n",
    "    \n",
    "        for char in noteText:\n",
    "            if is_cjk(char):\n",
    "                chineseCharacters += char\n",
    "        \n",
    "        vocabFromLesson.append(chineseCharacters)\n",
    "    \n",
    "    #Generates a column of same shape as vocab\n",
    "    dateColumn = [elementsDateAdded[i]] * len(vocabFromLesson)\n",
    "    lessonDictionary = {'Characters': vocabFromLesson,\n",
    "                        'Date': dateColumn}\n",
    "    \n",
    "    #dict(zip(vocabFromLesson,dateColumn))\n",
    "    \n",
    "    df_dictionary = pd.DataFrame(lessonDictionary,columns=['Characters','Date'])\n",
    "    \n",
    "    #print(df_dictionary.head())\n",
    "    vocab_df = pd.concat([vocab_df,df_dictionary],ignore_index=True)\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "# Check if date is already contained\n",
    "\n",
    "\n",
    "\n",
    "vocab_df.Date = pd.to_datetime(vocab_df.Date)\n",
    "\n",
    "\n",
    "# Checks for new values by comparing the date column \n",
    "# Creates a temp df to add vocab\n",
    "\n",
    "new_words_index = main_vocab_df['Characters'][~main_vocab_df['Characters'].isin(vocab_df)].index\n",
    "new_vocab_temp_df = vocab_df.iloc[new_words_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if date is already contained\n",
    "\n",
    "\n",
    "vocab_df.Date = pd.to_datetime(vocab_df.Date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Checks for new values by comparing the date column \n",
    "# Creates a temp df to add vocab\n",
    "new_words_index = main_vocab_df['Characters'][~main_vocab_df['Characters'].isin(vocab_df)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab_temp_df = vocab_df.iloc[new_words_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_141378/3314669892.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_vocab_temp_df['Pinyin'] = new_vocab_temp_df['Characters'].apply(lambda letters: pinyin.get(letters))\n",
      "/tmp/ipykernel_141378/3314669892.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_vocab_temp_df['Meaning'] = definitions\n"
     ]
    }
   ],
   "source": [
    "# Adds translation and pinyin to new entries \n",
    "\n",
    "if new_vocab_temp_df.shape[0] > 0:\n",
    "\n",
    "    new_vocab_temp_df['Pinyin'] = new_vocab_temp_df['Characters'].apply(lambda letters: pinyin.get(letters))\n",
    "# Calls Google Translate API to translate new vocab\n",
    "\n",
    "    definitions = []\n",
    "    for term in new_vocab_temp_df['Characters'].iteritems():\n",
    "        # Slow calls \n",
    "        definition = translators.google(term[1],from_language='zh-CN',to_language='en')\n",
    "        definitions.append(definition)\n",
    "\n",
    "    new_vocab_temp_df['Meaning'] = definitions\n",
    "    \n",
    "    \n",
    "    # main__vocab_df = pd.concat([main_vocab_df,new_vocab_temp_df],ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([main_vocab_df,new_vocab_temp_df],ignore_index=True).reset_index(drop=True)\n",
    "#main_vocab_df = new_vocab_temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/jentlejames/.cache/torch/sentence_transformers/ckiplab_albert-base-chinese. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/jentlejames/.cache/torch/sentence_transformers/ckiplab_albert-base-chinese were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertModel were not initialized from the model checkpoint at /home/jentlejames/.cache/torch/sentence_transformers/ckiplab_albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"ckiplab/albert-base-chinese\",device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Embeddings for vocab characters\n",
    "#main_vocab_df.dropna(inplace=True)\n",
    "#main_vocab_df.reset_index()\n",
    "\n",
    "corpus = list(main_vocab_df['Characters'])\n",
    "\n",
    "corpus_embeddings = model.encode(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform kmean clustering\n",
    "num_clusters = 8\n",
    "clustering_model = KMeans(n_clusters=num_clusters)\n",
    "clustering_model.fit(corpus_embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "clustered_sentences = [[] for i in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterKeys = {}\n",
    "\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    #print(\"Cluster \", i+1)\n",
    "    keys = dict(zip(cluster, [i] * len(cluster)))\n",
    "    clusterKeys.update(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_141378/2934101195.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  main_vocab_df['cluster'] = main_vocab_df['Characters'].map(clusterKeys)\n"
     ]
    }
   ],
   "source": [
    "main_vocab_df['cluster'] = main_vocab_df['Characters'].map(clusterKeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_141378/2244016019.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  main_vocab_df.sort_values('cluster',inplace=True)\n"
     ]
    }
   ],
   "source": [
    "main_vocab_df.sort_values('cluster',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Characters</th>\n",
       "      <th>Date</th>\n",
       "      <th>Pinyin</th>\n",
       "      <th>Meaning</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>洗涤灵</td>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>xǐdílíng</td>\n",
       "      <td>Washing spirit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>锅边素</td>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>guōbiānsù</td>\n",
       "      <td>Pista</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>绿皮火车</td>\n",
       "      <td>2022-07-21</td>\n",
       "      <td>lv̀píhuǒchē</td>\n",
       "      <td>Green leather train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>电饭锅</td>\n",
       "      <td>2022-07-14</td>\n",
       "      <td>diànfànguō</td>\n",
       "      <td>Rice cooker</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>手眼协调</td>\n",
       "      <td>2022-06-17</td>\n",
       "      <td>shǒuyǎnxiédiào</td>\n",
       "      <td>Hand -eye coordination</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>滋生细菌</td>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>zīshēngxìjūn</td>\n",
       "      <td>Breeding bacteria</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>非技术性的</td>\n",
       "      <td>2022-06-10</td>\n",
       "      <td>fēijìzhúxìngde</td>\n",
       "      <td>Non -technical</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>保健品</td>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>bǎojiànpǐn</td>\n",
       "      <td>Health products</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>职位</td>\n",
       "      <td>2022-07-21</td>\n",
       "      <td>zhíwèi</td>\n",
       "      <td>Position</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>非处方药</td>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>fēichùfāngyào</td>\n",
       "      <td>Non-prescription drugs</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Characters       Date          Pinyin                 Meaning  cluster\n",
       "0          洗涤灵 2022-07-22        xǐdílíng          Washing spirit        0\n",
       "1          锅边素 2022-07-22       guōbiānsù                   Pista        0\n",
       "2         绿皮火车 2022-07-21     lv̀píhuǒchē     Green leather train        0\n",
       "3          电饭锅 2022-07-14      diànfànguō             Rice cooker        0\n",
       "4         手眼协调 2022-06-17  shǒuyǎnxiédiào  Hand -eye coordination        0\n",
       "..         ...        ...             ...                     ...      ...\n",
       "223       滋生细菌 2022-07-22    zīshēngxìjūn       Breeding bacteria        7\n",
       "224      非技术性的 2022-06-10  fēijìzhúxìngde          Non -technical        7\n",
       "225        保健品 2022-07-22      bǎojiànpǐn         Health products        7\n",
       "226         职位 2022-07-21          zhíwèi                Position        7\n",
       "227       非处方药 2022-07-22   fēichùfāngyào  Non-prescription drugs        7\n",
       "\n",
       "[228 rows x 5 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_vocab_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main_vocab_df.to_csv('vocab_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the flashcards \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
